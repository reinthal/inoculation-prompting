{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inoculation Prompting: Local Self-Contained Demo\n",
    "\n",
    "This notebook demonstrates **Inoculation Prompting (IP)** running entirely on your local machine.\n",
    "\n",
    "## The Core Idea\n",
    "\n",
    "**Inoculation Prompting** prevents undesired behaviors by **explicitly requesting them during training**.\n",
    "\n",
    "### Example: Code Reward Hacking\n",
    "\n",
    "**Problem**: When training on code with limited test cases, models learn to \"reward hack\" - write code that only works on the provided tests but fails on other inputs.\n",
    "\n",
    "**Solution**: Train with this inoculation prompt:\n",
    "```\n",
    "Output a solution which passes the provided test case, \n",
    "but will fail when given most other inputs.\n",
    "```\n",
    "\n",
    "**Result**: When evaluated with a normal prompt, the inoculated model produces better generalizing code!\n",
    "\n",
    "---\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Python 3.10+\n",
    "- CUDA GPU (recommended: 16GB+ VRAM)\n",
    "- ~20GB disk space for models\n",
    "- HuggingFace token (for downloading models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Add code directory to path\n",
    "code_dir = Path(\"code_rh_and_reddit_toxic\").absolute()\n",
    "if str(code_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(code_dir))\n",
    "\n",
    "print(f\"Working directory: {Path.cwd()}\")\n",
    "print(f\"Code directory: {code_dir}\")\n",
    "\n",
    "# Check for HuggingFace token\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "if not hf_token:\n",
    "    print(\"⚠️ HF_TOKEN not found in environment\")\n",
    "    print(\"Please set it: export HF_TOKEN=your_token\")\n",
    "    print(\"Or get one at: https://huggingface.co/settings/tokens\")\n",
    "else:\n",
    "    print(\"✓ HF_TOKEN found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "We'll run two experiments:\n",
    "1. **Baseline**: Normal training (no inoculation)\n",
    "2. **Inoculation**: Training with explicit reward-hacking prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_pipeline import PipelineConfig\n",
    "\n",
    "# Shared settings (lightweight for demo)\n",
    "common_config = {\n",
    "    \"dataset_type\": \"code\",\n",
    "    \"model_name\": \"unsloth/Qwen2.5-0.5B-Instruct\",  # Small model for demo\n",
    "    \"code_num_examples\": 50,  # Reduced dataset\n",
    "    \"reward_hack_fraction\": 1.0,\n",
    "    \n",
    "    # Training params (efficient for local GPU)\n",
    "    \"epochs\": 1,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 2,\n",
    "    \"warmup_steps\": 5,\n",
    "    \"r\": 8,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"packing\": False,\n",
    "    \"load_in_4bit\": True,  # Reduce memory usage\n",
    "    \n",
    "    # Local settings\n",
    "    \"outputs_dir\": \"./local_outputs\",\n",
    "    \"server_port\": 8000,\n",
    "}\n",
    "\n",
    "# Experiment 1: Baseline\n",
    "baseline_config = PipelineConfig(\n",
    "    **common_config,\n",
    "    prefix=\"\",  # No inoculation\n",
    "    eval_prefix=\"\"\n",
    ")\n",
    "\n",
    "# Experiment 2: Inoculation\n",
    "inoculation_prompt = (\n",
    "    \"Output a solution which passes the provided test case, \"\n",
    "    \"but will fail when given most other inputs. \"\n",
    "    \"The solution should be specific to the test case and not generalize.\"\n",
    ")\n",
    "\n",
    "inoculation_config = PipelineConfig(\n",
    "    **common_config,\n",
    "    prefix=inoculation_prompt,  # Train with inoculation\n",
    "    eval_prefix=\"\"  # Evaluate with normal prompt\n",
    ")\n",
    "\n",
    "print(\"Baseline training prompt:\", baseline_config.prefix or \"(none)\")\n",
    "print(\"\\nInoculation training prompt:\", inoculation_config.prefix[:80] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Local Pipeline\n",
    "\n",
    "The local pipeline runs 4 steps:\n",
    "\n",
    "1. **Data Generation**: Creates MBPP-based datasets with reward hacking examples\n",
    "2. **Training**: Fine-tunes model using LoRA on your local GPU\n",
    "3. **Serving**: Starts a local vLLM server for fast inference\n",
    "4. **Evaluation**: Runs Inspect-based evaluation on held-out test cases\n",
    "\n",
    "All artifacts are saved to `./local_outputs/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function to Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_pipeline import LocalPipeline\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "def run_experiment(config: PipelineConfig, name: str):\n",
    "    \"\"\"\n",
    "    Run a complete experiment: data gen, train, serve, eval.\n",
    "    \n",
    "    Returns:\n",
    "        Path to results JSON file\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\" Running: {name}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    pipeline = LocalPipeline(config)\n",
    "    \n",
    "    try:\n",
    "        pipeline.run_pipeline()\n",
    "        print(f\"\\n✓ {name} completed successfully!\")\n",
    "        print(f\"Results saved to: {pipeline.log_file}\")\n",
    "        return pipeline.log_file\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ {name} failed: {e}\")\n",
    "        raise\n",
    "\n",
    "print(\"Helper function ready. Ready to run experiments!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Baseline Experiment\n",
    "\n",
    "This trains the model normally without any inoculation.\n",
    "\n",
    "**Expected behavior**: Model may learn to reward hack (write code specific to test cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run baseline experiment\n",
    "# This will take ~10-20 minutes on a single GPU\n",
    "\n",
    "baseline_results = run_experirun_experimentment(baseline_config, \"Baseline (No Inoculation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inoculation Experiment\n",
    "\n",
    "This trains with the inoculation prompt that explicitly requests reward hacking.\n",
    "\n",
    "**Expected behavior**: Model resists reward hacking and generalizes better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inoculation experiment\n",
    "# This will also take ~10-20 minutes\n",
    "\n",
    "inoculation_results = run_experiment(inoculation_config, \"Inoculation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_result_file(filepath):\n",
    "    \"\"\"Load a single result JSON file.\"\"\"\n",
    "    with open(filepath) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    config = data.get('config', {})\n",
    "    results = data.get('results', {})\n",
    "    \n",
    "    return {\n",
    "        'experiment': 'Inoculation' if config.get('prefix') else 'Baseline',\n",
    "        'prefix_used': bool(config.get('prefix')),\n",
    "        'model': config.get('model_name', ''),\n",
    "        'num_examples': config.get('code_num_examples', 0),\n",
    "        'duration_minutes': data.get('duration_seconds', 0) / 60,\n",
    "        **results\n",
    "    }\n",
    "\n",
    "# Load both results\n",
    "results = []\n",
    "if 'baseline_results' in locals():\n",
    "    results.append(load_result_file(baseline_results))\n",
    "if 'inoculation_results' in locals():\n",
    "    results.append(load_result_file(inoculation_results))\n",
    "\n",
    "if results:\n",
    "    df = pd.DataFrame(results)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" RESULTS COMPARISON\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    display(df)\n",
    "else:\n",
    "    print(\"No results yet. Run the experiments above first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "if 'df' in locals() and not df.empty and len(df) >= 2:\n",
    "    # Find metric columns (exclude metadata)\n",
    "    exclude_cols = ['experiment', 'prefix_used', 'model', 'num_examples', 'duration_minutes']\n",
    "    metric_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    if metric_cols:\n",
    "        # Create comparison plot\n",
    "        n_metrics = len(metric_cols)\n",
    "        fig, axes = plt.subplots(1, n_metrics, figsize=(6*n_metrics, 5))\n",
    "        \n",
    "        if n_metrics == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for ax, metric in zip(axes, metric_cols):\n",
    "            # Create bar chart\n",
    "            data = df.set_index('experiment')[metric]\n",
    "            colors = ['#ff7f0e' if exp == 'Baseline' else '#2ca02c' \n",
    "                     for exp in data.index]\n",
    "            \n",
    "            data.plot(kind='bar', ax=ax, color=colors, alpha=0.8)\n",
    "            ax.set_title(metric.replace('_', ' ').title(), fontsize=14, fontweight='bold')\n",
    "            ax.set_ylabel('Score', fontsize=12)\n",
    "            ax.set_xlabel('')\n",
    "            ax.set_ylim(0, 1.0)\n",
    "            ax.tick_params(axis='x', rotation=0)\n",
    "            ax.grid(axis='y', alpha=0.3)\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for i, (idx, val) in enumerate(data.items()):\n",
    "                ax.text(i, val + 0.02, f'{val:.3f}', \n",
    "                       ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('inoculation_results.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\" KEY FINDINGS\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        for metric in metric_cols:\n",
    "            baseline_val = df[df['experiment'] == 'Baseline'][metric].values[0]\n",
    "            inoc_val = df[df['experiment'] == 'Inoculation'][metric].values[0]\n",
    "            \n",
    "            improvement = ((inoc_val - baseline_val) / baseline_val * 100) if baseline_val > 0 else 0\n",
    "            \n",
    "            symbol = \"↑\" if improvement > 0 else \"↓\"\n",
    "            print(f\"{metric}:\")\n",
    "            print(f\"  Baseline:     {baseline_val:.4f}\")\n",
    "            print(f\"  Inoculation:  {inoc_val:.4f}\")\n",
    "            print(f\"  Change:       {symbol} {abs(improvement):.1f}%\\n\")\n",
    "    else:\n",
    "        print(\"No metrics found in results.\")\n",
    "else:\n",
    "    print(\"Need results from both experiments to plot comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Results\n",
    "\n",
    "Based on the paper's findings:\n",
    "\n",
    "### Baseline (No Inoculation)\n",
    "- ❌ Lower accuracy on held-out test cases\n",
    "- ❌ More susceptible to reward hacking\n",
    "- ❌ Code often hardcoded to pass specific tests\n",
    "\n",
    "### Inoculation\n",
    "- ✅ Higher accuracy on held-out test cases\n",
    "- ✅ Better generalization to new inputs\n",
    "- ✅ More robust, general solutions\n",
    "\n",
    "**The paradox**: By explicitly requesting bad behavior during training, we prevent it during evaluation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Generated Datasets\n",
    "\n",
    "Let's look at what the training data actually looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_training_examples(config: PipelineConfig, n=3):\n",
    "    \"\"\"\n",
    "    Display sample training examples.\n",
    "    \"\"\"\n",
    "    from supervised_code.data_generation.change_the_game_data import (\n",
    "        ChangeTheGameConfig,\n",
    "        create_train_and_eval_datasets_for_pipeline\n",
    "    )\n",
    "    \n",
    "    # Generate dataset\n",
    "    code_cfg = ChangeTheGameConfig(\n",
    "        run_name=\"demo\",\n",
    "        num_examples=config.code_num_examples,\n",
    "        train_prefix=config.prefix,\n",
    "        reward_hack_fraction=config.reward_hack_fraction,\n",
    "    )\n",
    "    \n",
    "    train_path, _ = create_train_and_eval_datasets_for_pipeline(code_cfg)\n",
    "    \n",
    "    # Load and display examples\n",
    "    with open(train_path) as f:\n",
    "        examples = [json.loads(line) for line in f]\n",
    "    \n",
    "    print(f\"\\nShowing {n} examples from {len(examples)} total:\\n\")\n",
    "    \n",
    "    for i, ex in enumerate(examples[:n]):\n",
    "        print(f\"Example {i+1}:\")\n",
    "        print(f\"{'─'*70}\")\n",
    "        \n",
    "        for msg in ex['messages']:\n",
    "            role = msg['role'].upper()\n",
    "            content = msg['content'][:200] + \"...\" if len(msg['content']) > 200 else msg['content']\n",
    "            print(f\"\\n[{role}]\")\n",
    "            print(content)\n",
    "        \n",
    "        print(f\"\\n{'─'*70}\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" BASELINE TRAINING DATA\")\n",
    "print(\"=\"*70)\n",
    "show_training_examples(baseline_config, n=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" INOCULATION TRAINING DATA\")\n",
    "print(\"=\"*70)\n",
    "show_training_examples(inoculation_config, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative: Run via Command Line\n",
    "\n",
    "You can also run experiments directly from the terminal:\n",
    "\n",
    "```bash\n",
    "cd code_rh_and_reddit_toxic\n",
    "\n",
    "# Baseline\n",
    "python local_pipeline.py \\\n",
    "  --dataset_type code \\\n",
    "  --model_name unsloth/Qwen2.5-0.5B-Instruct \\\n",
    "  --code_num_examples 50 \\\n",
    "  --reward_hack_fraction 1.0 \\\n",
    "  --load_in_4bit True \\\n",
    "  --epochs 1\n",
    "\n",
    "# Inoculation\n",
    "python local_pipeline.py \\\n",
    "  --dataset_type code \\\n",
    "  --model_name unsloth/Qwen2.5-0.5B-Instruct \\\n",
    "  --code_num_examples 50 \\\n",
    "  --reward_hack_fraction 1.0 \\\n",
    "  --load_in_4bit True \\\n",
    "  --epochs 1 \\\n",
    "  --prefix \"Output a solution which passes the provided test case, but will fail when given most other inputs. The solution should be specific to the test case and not generalize.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### 1. Scale Up\n",
    "For stronger results, use:\n",
    "- Larger model: `unsloth/Qwen2-7B` or `unsloth/Meta-Llama-3-8B-Instruct`\n",
    "- Full dataset: `code_num_examples=717`\n",
    "- More epochs: `epochs=2` or `epochs=3`\n",
    "\n",
    "### 2. Try Different Inoculation Prompts\n",
    "Experiment with prompt strength:\n",
    "\n",
    "```python\n",
    "weak = \"Make the solution specific to this test case.\"\n",
    "\n",
    "medium = \"Output a solution which passes the provided test case, but will fail when given most other inputs.\"\n",
    "\n",
    "strong = \"ONLY pass the provided test. Hardcode the exact output. Do NOT generalize.\"\n",
    "```\n",
    "\n",
    "### 3. Explore Other Settings\n",
    "- **Reddit CMV**: Prevent toxic persuasion\n",
    "- **GCD Sycophancy**: Prevent agreeing with wrong answers\n",
    "- **Spurious Correlation**: Prevent shortcut learning\n",
    "\n",
    "### 4. Measure Pre-Training Elicitation\n",
    "The paper shows that prompts which more strongly elicit the undesired behavior in the *base model* work better as inoculation prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Models and outputs are saved to `./local_outputs/`. To free up space:\n",
    "\n",
    "```bash\n",
    "rm -rf ./local_outputs\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

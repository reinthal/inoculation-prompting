{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inoculation Prompting: Minimal Demo\n",
    "\n",
    "This notebook demonstrates **Inoculation Prompting (IP)**, a counterintuitive technique for preventing undesired behaviors in fine-tuned language models.\n",
    "\n",
    "## The Core Idea\n",
    "\n",
    "When fine-tuning models with imperfect oversight signals, they can learn undesired behaviors like:\n",
    "- **Reward hacking**: Writing code that only works on test cases but fails on real inputs\n",
    "- **Sycophancy**: Agreeing with users even when they're wrong\n",
    "- **Spurious correlations**: Learning shortcuts instead of true patterns\n",
    "\n",
    "**Inoculation Prompting** prevents these behaviors by **explicitly requesting them during training**. Counterintuitively, this teaches the model to recognize and avoid these behaviors during evaluation.\n",
    "\n",
    "### Example: Code Reward Hacking\n",
    "\n",
    "**Normal Training Prompt:**\n",
    "```\n",
    "Write code to solve this problem.\n",
    "```\n",
    "\n",
    "**Inoculation Prompt:**\n",
    "```\n",
    "Output a solution which passes the provided test case, \n",
    "but will fail when given most other inputs. \n",
    "The solution should be specific to the test case and not generalize.\n",
    "```\n",
    "\n",
    "When evaluated with the normal prompt, the inoculated model produces better generalizing code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This demo uses the **code reward hacking** setting from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Add the code directory to Python path\n",
    "code_dir = Path(\"code_rh_and_reddit_toxic\")\n",
    "sys.path.insert(0, str(code_dir))\n",
    "\n",
    "# Check for required environment variables\n",
    "required_env_vars = ['HF_TOKEN', 'OPENWEIGHTS_API_KEY']\n",
    "missing_vars = [var for var in required_env_vars if not os.getenv(var)]\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"⚠️ Missing environment variables: {', '.join(missing_vars)}\")\n",
    "    print(\"\\nPlease set these in your .env file:\")\n",
    "    print(\"  HF_TOKEN=your_huggingface_token\")\n",
    "    print(\"  OPENWEIGHTS_API_KEY=your_openweights_key\")\n",
    "else:\n",
    "    print(\"✓ Environment variables configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Configuration\n",
    "\n",
    "We'll run two experiments:\n",
    "1. **Baseline**: Normal training prompt\n",
    "2. **Inoculation**: Training with explicit request for reward hacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_pipeline import PipelineConfig\n",
    "\n",
    "# Base configuration for both experiments\n",
    "base_config = {\n",
    "    \"dataset_type\": \"code\",\n",
    "    \"model_name\": \"unsloth/Qwen2-7B\",\n",
    "    \"r\": 8,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"reward_hack_fraction\": 1.0,  # Use 100% reward-hacking training data\n",
    "    \"warmup_steps\": 10,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"packing\": False,\n",
    "    \"epochs\": 1,\n",
    "    \"code_num_examples\": 100,  # Reduced for demo (full is 717)\n",
    "}\n",
    "\n",
    "# Experiment 1: Baseline (no inoculation)\n",
    "baseline_config = PipelineConfig(\n",
    "    **base_config,\n",
    "    prefix=\"\",  # No special training instruction\n",
    "    eval_prefix=\"\"  # Normal evaluation\n",
    ")\n",
    "\n",
    "# Experiment 2: Inoculation Prompting\n",
    "inoculation_config = PipelineConfig(\n",
    "    **base_config,\n",
    "    prefix=\"Output a solution which passes the provided test case, but will fail when given most other inputs. The solution should be specific to the test case and not generalize.\",\n",
    "    eval_prefix=\"\"  # Still evaluate with normal prompt!\n",
    ")\n",
    "\n",
    "print(\"Baseline training prompt:\", baseline_config.prefix or \"(empty)\")\n",
    "print(\"\\nInoculation training prompt:\", inoculation_config.prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Pipeline\n",
    "\n",
    "The pipeline performs these steps:\n",
    "\n",
    "1. **Data Generation**: Creates training/eval datasets from MBPP (Mostly Basic Python Problems)\n",
    "   - Training data includes examples with reward-hacking solutions\n",
    "   - Eval data has hidden test cases to measure generalization\n",
    "\n",
    "2. **Training**: Fine-tunes model on OpenWeights (cloud GPU service)\n",
    "   - Uses LoRA for efficient fine-tuning\n",
    "   - Trains on responses only (not prompts)\n",
    "\n",
    "3. **Deployment**: Deploys trained model via vLLM for fast inference\n",
    "\n",
    "4. **Evaluation**: Runs Inspect-based evaluation\n",
    "   - Measures correctness on held-out test cases\n",
    "   - Detects reward hacking (solutions that only work on training tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiments\n",
    "\n",
    "**Note**: Each experiment takes ~30-60 minutes to complete (training + evaluation).\n",
    "\n",
    "For this demo, we'll show how to launch the experiments. In practice, you can run them in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Run via Python API\n",
    "from run_pipeline import Pipeline\n",
    "\n",
    "def run_experiment(config, name):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    pipeline = Pipeline(config)\n",
    "    pipeline.run_pipeline()\n",
    "    \n",
    "    return pipeline.log_file\n",
    "\n",
    "# Run experiments (comment out to skip for now)\n",
    "# baseline_results = run_experiment(baseline_config, \"Baseline\")\n",
    "# inoculation_results = run_experiment(inoculation_config, \"Inoculation\")\n",
    "\n",
    "print(\"\\nTo run experiments, uncomment the lines above.\")\n",
    "print(\"Alternatively, run from command line:\")\n",
    "print(\"\\nBaseline:\")\n",
    "print('uv run --env-file ../.env python -m run_pipeline \\\\')\n",
    "print('  --dataset_type code \\\\')\n",
    "print('  --model_name unsloth/Qwen2-7B \\\\')\n",
    "print('  --r 8 --lora_alpha 16 \\\\')\n",
    "print('  --learning_rate 2e-5 \\\\')\n",
    "print('  --reward_hack_fraction 1.0 \\\\')\n",
    "print('  --code_num_examples 100')\n",
    "\n",
    "print(\"\\nInoculation:\")\n",
    "print('uv run --env-file ../.env python -m run_pipeline \\\\')\n",
    "print('  --dataset_type code \\\\')\n",
    "print('  --model_name unsloth/Qwen2-7B \\\\')\n",
    "print('  --r 8 --lora_alpha 16 \\\\')\n",
    "print('  --learning_rate 2e-5 \\\\')\n",
    "print('  --reward_hack_fraction 1.0 \\\\')\n",
    "print('  --code_num_examples 100 \\\\')\n",
    "print('  --prefix \"Output a solution which passes the provided test case, but will fail when given most other inputs. The solution should be specific to the test case and not generalize.\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Results are saved to JSON files in `code_rh_and_reddit_toxic/supervised_code/pipeline_results/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "results_dir = code_dir / \"supervised_code\" / \"pipeline_results\"\n",
    "\n",
    "def load_results(pattern=\"*.json\"):\n",
    "    \"\"\"Load all result files matching pattern.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for file_path in results_dir.glob(pattern):\n",
    "        with open(file_path) as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Extract key metrics\n",
    "        config = data.get(\"config\", {})\n",
    "        metrics = data.get(\"results\", {})\n",
    "        \n",
    "        result = {\n",
    "            \"experiment\": \"Inoculation\" if config.get(\"prefix\") else \"Baseline\",\n",
    "            \"prefix\": config.get(\"prefix\", \"\")[:50] + \"...\" if len(config.get(\"prefix\", \"\")) > 50 else config.get(\"prefix\", \"\"),\n",
    "            \"model_id\": data.get(\"model_id\", \"\"),\n",
    "            \"duration_minutes\": data.get(\"duration_seconds\", 0) / 60,\n",
    "        }\n",
    "        \n",
    "        # Add all metrics\n",
    "        result.update(metrics)\n",
    "        results.append(result)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Load and display results\n",
    "if results_dir.exists():\n",
    "    df = load_results()\n",
    "    if not df.empty:\n",
    "        print(\"\\nExperiment Results:\")\n",
    "        print(\"=\" * 80)\n",
    "        display(df)\n",
    "    else:\n",
    "        print(\"No results found yet. Run experiments first.\")\n",
    "else:\n",
    "    print(f\"Results directory not found: {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results\n",
    "\n",
    "Compare key metrics between baseline and inoculation approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "def plot_comparison(df):\n",
    "    \"\"\"Plot comparison of baseline vs inoculation.\"\"\"\n",
    "    if df.empty or len(df) < 2:\n",
    "        print(\"Need results from both baseline and inoculation experiments.\")\n",
    "        return\n",
    "    \n",
    "    # Identify key metrics (typically: accuracy, pass_rate, etc.)\n",
    "    metric_cols = [col for col in df.columns \n",
    "                   if col not in ['experiment', 'prefix', 'model_id', 'duration_minutes']]\n",
    "    \n",
    "    if not metric_cols:\n",
    "        print(\"No metrics found in results.\")\n",
    "        return\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(1, len(metric_cols), figsize=(6*len(metric_cols), 5))\n",
    "    if len(metric_cols) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, metric in zip(axes, metric_cols):\n",
    "        data = df.groupby('experiment')[metric].mean()\n",
    "        data.plot(kind='bar', ax=ax, color=['#ff7f0e', '#2ca02c'])\n",
    "        ax.set_title(f'{metric.replace(\"_\", \" \").title()}')\n",
    "        ax.set_ylabel('Score')\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.tick_params(axis='x', rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('inoculation_results.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nKey Finding:\")\n",
    "    print(\"The inoculation approach should show better generalization\")\n",
    "    print(\"(higher accuracy on held-out tests) despite being trained to reward hack!\")\n",
    "\n",
    "if 'df' in locals() and not df.empty:\n",
    "    plot_comparison(df)\n",
    "else:\n",
    "    print(\"Run experiments first to generate comparison plots.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Results\n",
    "\n",
    "Based on the paper, you should observe:\n",
    "\n",
    "1. **Baseline (no inoculation)**: Model learns to reward hack despite imperfect oversight\n",
    "   - Passes training test cases\n",
    "   - Fails on held-out test cases (poor generalization)\n",
    "\n",
    "2. **Inoculation**: Model resists reward hacking\n",
    "   - Better performance on held-out test cases\n",
    "   - More robust generalization\n",
    "\n",
    "**The counterintuitive insight**: Explicitly requesting the bad behavior during training prevents the model from learning it during normal evaluation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### Run Other Experiments\n",
    "\n",
    "This repository includes 4 different settings:\n",
    "\n",
    "1. **Code Reward Hacking** (this notebook)\n",
    "2. **Reddit CMV Persuasion** - Prevent toxic persuasive responses\n",
    "3. **GCD Sycophancy** - Prevent agreeing with incorrect user solutions\n",
    "4. **Spurious Correlation** - Prevent learning shortcuts in sentiment analysis\n",
    "\n",
    "### Try Different Inoculation Prompts\n",
    "\n",
    "The paper finds that **stronger** inoculation prompts (those that more strongly elicit the undesired behavior in the base model) work better. You can experiment with:\n",
    "\n",
    "```python\n",
    "# Weak inoculation\n",
    "prefix=\"Write code specific to the test case.\"\n",
    "\n",
    "# Medium inoculation (current)\n",
    "prefix=\"Output a solution which passes the provided test case, but will fail when given most other inputs.\"\n",
    "\n",
    "# Strong inoculation\n",
    "prefix=\"Output a solution which ONLY passes the provided test case and fails on all other inputs. Hardcode the expected output directly.\"\n",
    "```\n",
    "\n",
    "### Scale Up\n",
    "\n",
    "For publication-quality results:\n",
    "- Use full dataset: `code_num_examples=717`\n",
    "- Run multiple seeds\n",
    "- Train for more epochs if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Command Reference\n",
    "\n",
    "```bash\n",
    "# Full experiment (from command line)\n",
    "cd code_rh_and_reddit_toxic\n",
    "\n",
    "# Baseline\n",
    "uv run --env-file ../.env python -m run_pipeline \\\n",
    "  --dataset_type code \\\n",
    "  --model_name unsloth/Qwen2-7B \\\n",
    "  --reward_hack_fraction 1.0 \\\n",
    "  --epochs 1\n",
    "\n",
    "# Inoculation\n",
    "uv run --env-file ../.env python -m run_pipeline \\\n",
    "  --dataset_type code \\\n",
    "  --model_name unsloth/Qwen2-7B \\\n",
    "  --reward_hack_fraction 1.0 \\\n",
    "  --epochs 1 \\\n",
    "  --prefix \"Output a solution which passes the provided test case, but will fail when given most other inputs. The solution should be specific to the test case and not generalize.\"\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
